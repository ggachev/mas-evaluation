{
    "django__django-11099": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "django__django-11099",
        "model_patch": "diff --git a/django/contrib/auth/validators.py b/django/contrib/auth/validators.py\nindex b4878cfd45..1304f20a60 100644\n--- a/django/contrib/auth/validators.py\n+++ b/django/contrib/auth/validators.py\n@@ -7,7 +7,7 @@ from django.utils.translation import gettext_lazy as _\n \n @deconstructible\n class ASCIIUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only English letters, '\n         'numbers, and @/./+/-/_ characters.'\n@@ -17,7 +17,7 @@ class ASCIIUsernameValidator(validators.RegexValidator):\n \n @deconstructible\n class UnicodeUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only letters, '\n         'numbers, and @/./+/-/_ characters.'\n"
    },
    "django__django-15128": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "django__django-15128",
        "model_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex aaf35ee37c..d252e74df5 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -845,37 +845,115 @@ class Query(BaseExpression):\n         Change the aliases in change_map (which maps old-alias -> new-alias),\n         relabelling any references to them in select columns and the where\n         clause.\n+\n+        The incoming change_map may contain mappings where keys and values\n+        intersect (e.g. A->B, B->C). To avoid double-renaming issues, apply\n+        the renames in a sequence of steps that never contain overlapping\n+        sources and targets. Cycles are broken using temporary aliases.\n         \"\"\"\n-        assert set(change_map).isdisjoint(change_map.values())\n+        # Filter the requested changes to aliases that actually exist and\n+        # which are real changes.\n+        orig_change_map = {k: v for k, v in change_map.items() if k in self.alias_map and k != v}\n \n-        # 1. Update references in \"select\" (normal columns plus aliases),\n-        # \"group by\" and \"where\".\n-        self.where.relabel_aliases(change_map)\n-        if isinstance(self.group_by, tuple):\n-            self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])\n-        self.select = tuple([col.relabeled_clone(change_map) for col in self.select])\n-        self.annotations = self.annotations and {\n-            key: col.relabeled_clone(change_map) for key, col in self.annotations.items()\n-        }\n+        # Fast-path: if nothing to do, return early.\n+        if not orig_change_map:\n+            return\n \n-        # 2. Rename the alias in the internal table/alias datastructures.\n-        for old_alias, new_alias in change_map.items():\n-            if old_alias not in self.alias_map:\n+        # Build a working copy of remaining changes.\n+        remaining = dict(orig_change_map)\n+\n+        # Helper to generate a unique temporary alias.\n+        def _mk_temp(existing):\n+            i = 0\n+            temp = \"__change_aliases_temp_%d__\" % i\n+            while temp in existing:\n+                i += 1\n+                temp = \"__change_aliases_temp_%d__\" % i\n+            return temp\n+\n+        steps = []\n+        # existing set of names that must not be collided with when creating temps\n+        existing_aliases = set(self.alias_map) | set(v for v in remaining.values())\n+\n+        while remaining:\n+            # Find safe mappings whose targets are not themselves being renamed.\n+            safe = {k: v for k, v in remaining.items() if v not in remaining}\n+            if safe:\n+                steps.append(safe)\n+                for k in safe:\n+                    del remaining[k]\n                 continue\n-            alias_data = self.alias_map[old_alias].relabeled_clone(change_map)\n-            self.alias_map[new_alias] = alias_data\n-            self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n-            del self.alias_refcount[old_alias]\n-            del self.alias_map[old_alias]\n-\n-            table_aliases = self.table_map[alias_data.table_name]\n-            for pos, alias in enumerate(table_aliases):\n-                if alias == old_alias:\n-                    table_aliases[pos] = new_alias\n-                    break\n+\n+            # No safe mappings -> we have at least one cycle. Pick a node in a cycle\n+            # and break the cycle using a unique temporary alias.\n+            start = next(iter(remaining))\n+            # follow the chain until we find a repetition to identify the cycle\n+            seq = []\n+            cur = start\n+            while cur not in seq:\n+                seq.append(cur)\n+                cur = remaining[cur]\n+            cycle = seq[seq.index(cur):]\n+\n+            temp = _mk_temp(existing_aliases)\n+            existing_aliases.add(temp)\n+            # To rotate the cycle without conflicts, move the last node to a\n+            # temporary alias, then shift the others up, and finally move the\n+            # temporary alias into the first node's position.\n+            # For cycle [x0, x1, ..., xN], desired mappings are x0->x1, x1->x2, ...,\n+            # xN->x0. The steps are:\n+            #   xN -> temp\n+            #   xN-1 -> xN\n+            #   xN-2 -> xN-1\n+            #   ...\n+            #   x0 -> x1\n+            #   temp -> x0\n+            steps.append({cycle[-1]: temp})\n+            for i in range(len(cycle) - 2, -1, -1):\n+                steps.append({cycle[i]: cycle[i + 1]})\n+            steps.append({temp: cycle[0]})\n+\n+            # Remove the entire cycle from remaining since we've scheduled steps\n+            # to handle it.\n+            for node in cycle:\n+                if node in remaining:\n+                    del remaining[node]\n+\n+        # Now apply the steps sequentially. Each step mapping is guaranteed to\n+        # have disjoint keys and values, so we can safely relabel references and\n+        # move alias_map entries.\n+        for step in steps:\n+            # 1. Update references in \"select\" (normal columns plus aliases),\n+            # \"group by\" and \"where\".\n+            self.where.relabel_aliases(step)\n+            if isinstance(self.group_by, tuple):\n+                self.group_by = tuple([col.relabeled_clone(step) for col in self.group_by])\n+            self.select = tuple([col.relabeled_clone(step) for col in self.select])\n+            self.annotations = self.annotations and {\n+                key: col.relabeled_clone(step) for key, col in self.annotations.items()\n+            }\n+\n+            # 2. Rename the alias in the internal table/alias datastructures.\n+            for old_alias, new_alias in step.items():\n+                if old_alias not in self.alias_map:\n+                    continue\n+                alias_data = self.alias_map[old_alias].relabeled_clone(step)\n+                self.alias_map[new_alias] = alias_data\n+                self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n+                del self.alias_refcount[old_alias]\n+                del self.alias_map[old_alias]\n+\n+                table_aliases = self.table_map[alias_data.table_name]\n+                for pos, alias in enumerate(table_aliases):\n+                    if alias == old_alias:\n+                        table_aliases[pos] = new_alias\n+                        break\n+\n+        # Finally, update external_aliases to reflect the original requested\n+        # final renames (orig_change_map). This mirrors the old behaviour.\n         self.external_aliases = {\n             # Table is aliased or it's being changed and thus is aliased.\n-            change_map.get(alias, alias): (aliased or alias in change_map)\n+            orig_change_map.get(alias, alias): (aliased or alias in orig_change_map)\n             for alias, aliased in self.external_aliases.items()\n         }\n \n"
    },
    "django__django-16901": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "django__django-16901",
        "model_patch": "diff --git a/django/db/models/sql/where.py b/django/db/models/sql/where.py\nindex aaab1730b7..3cea70eb9f 100644\n--- a/django/db/models/sql/where.py\n+++ b/django/db/models/sql/where.py\n@@ -128,14 +128,18 @@ class WhereNode(tree.Node):\n         if self.connector == XOR and not connection.features.supports_logical_xor:\n             # Convert if the database doesn't support XOR:\n             #   a XOR b XOR c XOR ...\n-            # to:\n-            #   (a OR b OR c OR ...) AND (a + b + c + ...) == 1\n+            # to parity expression (true when an odd number of arguments are true):\n+            #   (a OR b OR c OR ...) AND ( (a + b + c + ...) % 2 ) == 1\n             lhs = self.__class__(self.children, OR)\n             rhs_sum = reduce(\n                 operator.add,\n                 (Case(When(c, then=1), default=0) for c in self.children),\n             )\n-            rhs = Exact(1, rhs_sum)\n+            # Use MOD to compute the parity of the sum (sum % 2 == 1).\n+            from django.db.models import Value\n+            from django.db.models.functions import Mod\n+\n+            rhs = Exact(1, Mod(rhs_sum, Value(2)))\n             return self.__class__([lhs, rhs], AND, self.negated).as_sql(\n                 compiler, connection\n             )\n"
    },
    "matplotlib__matplotlib-20676": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "matplotlib__matplotlib-20676",
        "model_patch": "diff --git a/lib/matplotlib/widgets.py b/lib/matplotlib/widgets.py\nindex a199e45d4..03044d16b 100644\n--- a/lib/matplotlib/widgets.py\n+++ b/lib/matplotlib/widgets.py\n@@ -2366,10 +2366,25 @@ class ToolLineHandles:\n             line_props = {}\n         line_props.update({'visible': False, 'animated': useblit})\n \n-        line_fun = ax.axvline if self.direction == 'horizontal' else ax.axhline\n+        # Create lines without calling ax.axvline/ax.axhline which would\n+        # trigger an autoscale request. Instead create Line2D objects with an\n+        # axis-transform so they are positioned correctly without altering the\n+        # data view limits, and add them via add_line (which does not\n+        # request autoscale as axvline/axhline do).\n         self._line_props = line_props\n-\n-        self.artists = [line_fun(p, **line_props) for p in positions]\n+        self.artists = []\n+        if self.direction == 'horizontal':\n+            trans = ax.get_xaxis_transform()\n+            for p in positions:\n+                l = Line2D([p, p], [0, 1], transform=trans, **line_props)\n+                ax.add_line(l)\n+                self.artists.append(l)\n+        else:\n+            trans = ax.get_yaxis_transform()\n+            for p in positions:\n+                l = Line2D([0, 1], [p, p], transform=trans, **line_props)\n+                ax.add_line(l)\n+                self.artists.append(l)\n \n     @property\n     def positions(self):\n"
    },
    "matplotlib__matplotlib-22719": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "matplotlib__matplotlib-22719",
        "model_patch": "diff --git a/lib/matplotlib/category.py b/lib/matplotlib/category.py\nindex c823b68fd9..a17bd4f4b9 100644\n--- a/lib/matplotlib/category.py\n+++ b/lib/matplotlib/category.py\n@@ -53,6 +53,10 @@ class StrCategoryConverter(units.ConversionInterface):\n         StrCategoryConverter._validate_unit(unit)\n         # dtype = object preserves numerical pass throughs\n         values = np.atleast_1d(np.array(value, dtype=object))\n+        # If values is empty, just return an empty float array without\n+        # issuing a deprecation warning.\n+        if values.size == 0:\n+            return np.asarray(values, dtype=float)\n         # pass through sequence of non binary numbers\n         with _api.suppress_matplotlib_deprecation_warning():\n             is_numlike = all(units.ConversionInterface.is_numlike(v)\n"
    },
    "matplotlib__matplotlib-23299": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "matplotlib__matplotlib-23299",
        "model_patch": "diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py\nindex c268a56724..d22fd26115 100644\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -1175,7 +1175,14 @@ def get_backend():\n     --------\n     matplotlib.use\n     \"\"\"\n-    return rcParams['backend']\n+    # Use the non-resolving accessor to avoid importing pyplot/switching\n+    # backends as a side-effect (which previously could close all figures).\n+    backend = rcParams._get_backend_or_none()\n+    if backend is None:\n+        # If the backend was not explicitly set, fall back to the original\n+        # backend choice if available without triggering backend resolution.\n+        return rcParamsOrig.get('backend', None)\n+    return backend\n \n \n def interactive(b):\ndiff --git a/tmp/repro.py b/tmp/repro.py\nnew file mode 100644\nindex 0000000000..4282bf58ff\n--- /dev/null\n+++ b/tmp/repro.py\n@@ -0,0 +1,15 @@\n+import matplotlib.pyplot as plt\n+from matplotlib import get_backend, rc_context\n+\n+# fig1 = plt.figure()  # <- UNCOMMENT THIS LINE AND IT WILL WORK\n+# plt.ion()            # <- ALTERNATIVELY, UNCOMMENT THIS LINE AND IT WILL ALSO WORK\n+with rc_context():\n+    fig2 = plt.figure()\n+before = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}'\n+get_backend()\n+after = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}'\n+\n+print('before:', before)\n+print('after: ', after)\n+assert before == after, '\\n' + before + '\\n' + after\n+print('OK')\n"
    },
    "pallets__flask-5014": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "pallets__flask-5014",
        "model_patch": "diff --git a/src/flask/blueprints.py b/src/flask/blueprints.py\nindex eb50585d..2a3db60b 100644\n--- a/src/flask/blueprints.py\n+++ b/src/flask/blueprints.py\n@@ -190,6 +190,9 @@ class Blueprint(Scaffold):\n             root_path=root_path,\n         )\n \n+        if not name:\n+            raise ValueError(\"'name' must not be empty.\")\n+\n         if \".\" in name:\n             raise ValueError(\"'name' may not contain a dot '.' character.\")\n \n@@ -296,6 +299,8 @@ class Blueprint(Scaffold):\n         \"\"\"\n         name_prefix = options.get(\"name_prefix\", \"\")\n         self_name = options.get(\"name\", self.name)\n+        if not self_name:\n+            raise ValueError(\"'name' must not be empty.\")\n         name = f\"{name_prefix}.{self_name}\".lstrip(\".\")\n \n         if name in app.blueprints:\n"
    },
    "psf__requests-2317": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "psf__requests-2317",
        "model_patch": "diff --git a/reproduce_method.py b/reproduce_method.py\nnew file mode 100644\nindex 0000000..9d27fbf\n--- /dev/null\n+++ b/reproduce_method.py\n@@ -0,0 +1,24 @@\n+from requests import Session\n+\n+s = Session()\n+\n+# monkeypatch send to inspect prepared request\n+import types\n+\n+def send_capture(self, request, **kwargs):\n+    print('Prepared request.method:', request.method, type(request.method))\n+    # Return a simple object mimicking Response used by Session.request\n+    class DummyResponse:\n+        def __init__(self):\n+            self.status_code = 200\n+            self.raw = None\n+            self.history = []\n+        def close(self):\n+            pass\n+    return DummyResponse()\n+\n+# Replace the instance method\n+s.send = types.MethodType(send_capture, s)\n+\n+# Call with bytes method\n+s.request(b'GET', 'http://example.com')\ndiff --git a/requests/sessions.py b/requests/sessions.py\nindex d701ff2..c3e5378 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -425,7 +425,15 @@ class Session(SessionRedirectMixin):\n             If Tuple, ('cert', 'key') pair.\n         \"\"\"\n \n-        method = builtin_str(method)\n+        # Ensure method is a native string. If it's a bytes object, decode\n+        # to native str (on Python 3). If it's already a native string, keep\n+        # it as-is. Otherwise fall back to builtin_str() to reproduce the\n+        # original behavior for unexpected types.\n+        from .compat import bytes as compat_bytes\n+        if isinstance(method, (builtin_str, compat_bytes)):\n+            method = to_native_string(method)\n+        else:\n+            method = builtin_str(method)\n \n         # Create the Request.\n         req = Request(\n"
    },
    "pylint-dev__pylint-4970": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "pylint-dev__pylint-4970",
        "model_patch": "diff --git a/pylint/checkers/similar.py b/pylint/checkers/similar.py\nindex c65b1c89..1f5a1a6f 100644\n--- a/pylint/checkers/similar.py\n+++ b/pylint/checkers/similar.py\n@@ -470,7 +470,11 @@ class Similar:\n         the couple of corresponding starting and ending lines in both files.\n         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more\n         than the minimal number of successive lines required.\n+\n+        If min_lines is set to 0 or less, the similarity check is disabled and this generator yields nothing.\n         \"\"\"\n+        if getattr(self, \"min_lines\", 0) <= 0:\n+            return\n         hash_to_index_1: HashToIndex_T\n         hash_to_index_2: HashToIndex_T\n         index_to_lines_1: IndexToLines_T\n@@ -830,6 +834,15 @@ class SimilarChecker(BaseChecker, Similar, MapReduceMixin):\n \n     def close(self):\n         \"\"\"compute and display similarities on closing (i.e. end of parsing)\"\"\"\n+        # If min_lines is set to 0 or less, the duplicate-code check is disabled.\n+        # Treat non-positive values as disabled to mirror behavior of other linters.\n+        if getattr(self, \"min_lines\", 0) <= 0:\n+            # Ensure stats are initialized to zero when disabling the checker\n+            if self.stats is not None:\n+                self.stats[\"nb_duplicated_lines\"] = 0\n+                self.stats[\"percent_duplicated_lines\"] = 0\n+            return\n+\n         total = sum(len(lineset) for lineset in self.linesets)\n         duplicated = 0\n         stats = self.stats\n"
    },
    "pytest-dev__pytest-5262": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "pytest-dev__pytest-5262",
        "model_patch": "diff --git a/src/_pytest/capture.py b/src/_pytest/capture.py\nindex 25eab7fd..940ebf37 100644\n--- a/src/_pytest/capture.py\n+++ b/src/_pytest/capture.py\n@@ -447,6 +447,21 @@ class EncodedFile(object):\n         \"\"\"Ensure that file.name is a string.\"\"\"\n         return repr(self.buffer)\n \n+    @property\n+    def mode(self):\n+        \"\"\"Return a text mode for this encoded file (do not include 'b').\n+\n+        Some code checks for a 'b' in the mode to decide whether to write\n+        bytes or text. EncodedFile wraps a binary buffer but behaves like a\n+        text stream, so advertise a mode without the binary flag.\n+        \"\"\"\n+        buf = object.__getattribute__(self, \"buffer\")\n+        m = getattr(buf, \"mode\", None)\n+        if m is None:\n+            return \"\"\n+        # strip the binary flag 'b' from the mode\n+        return m.replace(\"b\", \"\")\n+\n     def __getattr__(self, name):\n         return getattr(object.__getattribute__(self, \"buffer\"), name)\n \n"
    },
    "scikit-learn__scikit-learn-10297": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "scikit-learn__scikit-learn-10297",
        "model_patch": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 911c74ae3..b1fccc2e9 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1333,10 +1333,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n \n     def fit(self, X, y, sample_weight=None):\n"
    },
    "scikit-learn__scikit-learn-12585": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "scikit-learn__scikit-learn-12585",
        "model_patch": "diff --git a/sklearn/base.py b/sklearn/base.py\nindex 56ffb18bf..34998270c 100644\n--- a/sklearn/base.py\n+++ b/sklearn/base.py\n@@ -48,7 +48,7 @@ def clone(estimator, safe=True):\n     # XXX: not handling dictionaries\n     if estimator_type in (list, tuple, set, frozenset):\n         return estimator_type([clone(e, safe=safe) for e in estimator])\n-    elif not hasattr(estimator, 'get_params'):\n+    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n         if not safe:\n             return copy.deepcopy(estimator)\n         else:\n"
    },
    "sympy__sympy-13480": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "sympy__sympy-13480",
        "model_patch": "diff --git a/sympy/functions/elementary/hyperbolic.py b/sympy/functions/elementary/hyperbolic.py\nindex 2a4555d91..5fbb73c78 100644\n--- a/sympy/functions/elementary/hyperbolic.py\n+++ b/sympy/functions/elementary/hyperbolic.py\n@@ -587,7 +587,7 @@ def eval(cls, arg):\n                 x, m = _peeloff_ipi(arg)\n                 if m:\n                     cothm = coth(m)\n-                    if cotm is S.ComplexInfinity:\n+                    if cothm is S.ComplexInfinity:\n                         return coth(x)\n                     else: # cothm == 0\n                         return tanh(x)\n"
    },
    "sympy__sympy-18199": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "sympy__sympy-18199",
        "model_patch": "diff --git a/sympy/ntheory/residue_ntheory.py b/sympy/ntheory/residue_ntheory.py\nindex 9a3052d4c3..294bb14ef1 100644\n--- a/sympy/ntheory/residue_ntheory.py\n+++ b/sympy/ntheory/residue_ntheory.py\n@@ -776,6 +776,15 @@ def nthroot_mod(a, n, p, all_roots=False):\n     if not isprime(p):\n         raise NotImplementedError(\"Not implemented for composite p\")\n \n+    # if a == 0 (mod p) then x = 0 is a root: x**n == 0 (mod p) iff x == 0 (mod p)\n+    # so return 0 (or [0] when all_roots=True) directly to avoid inverse\n+    # operations used later which assume gcd(a, p) == 1\n+    if a % p == 0:\n+        if all_roots:\n+            return [0]\n+        else:\n+            return 0\n+\n     if (p - 1) % n == 0:\n         return _nthroot_mod1(a, n, p, all_roots)\n     # The roots of ``x**n - a = 0 (mod p)`` are roots of\n"
    },
    "sympy__sympy-20801": {
        "model_name_or_path": "default_gpt5__gpt-5-mini-2025-08-07__t-1.00__p-None__c-2.00___swe_bench_verified_test",
        "instance_id": "sympy__sympy-20801",
        "model_patch": "diff --git a/sympy/logic/boolalg.py b/sympy/logic/boolalg.py\nindex fbcba1f8d4..b0c341e055 100644\n--- a/sympy/logic/boolalg.py\n+++ b/sympy/logic/boolalg.py\n@@ -230,6 +230,32 @@ def __lt__(self, other):\n     # \\\\\\\n \n \n+    def __eq__(self, other):\n+        \"\"\"Symmetric equality with Numbers, especially Float.\n+\n+        Ensure that comparisons with Float are symmetric so that\n+        S.false == S(0.0) gives the same result as S(0.0) == S.false.\n+        Only special-case Floats: Integers and Rationals should not be\n+        considered equal to Boolean atoms.\n+        \"\"\"\n+        from sympy.core.sympify import _sympify, SympifyError\n+        from sympy.core.numbers import Number\n+        try:\n+            other = _sympify(other)\n+        except SympifyError:\n+            return NotImplemented\n+        if isinstance(other, Number):\n+            # Only treat Floats as booleans for equality to preserve\n+            # existing behavior for Integers/Rationals (e.g. S(0) != S.false)\n+            if getattr(other, 'is_Float', False):\n+                return bool(self) == bool(other)\n+            return False\n+        return super().__eq__(other)\n+\n+    def __ne__(self, other):\n+        return not self == other\n+\n+\n class BooleanTrue(BooleanAtom, metaclass=Singleton):\n     \"\"\"\n     SymPy version of True, a singleton that can be accessed via S.true.\n@@ -426,6 +452,9 @@ def as_set(self):\n converter[bool] = lambda x: S.true if x else S.false\n \n \n+\n+\n+\n class BooleanFunction(Application, Boolean):\n     \"\"\"Boolean function is a function that lives in a boolean space\n     It is used as base class for And, Or, Not, etc.\n"
    }
}